---
title: 'Automatic Differentiation'
date: 2025-04-16
permalink: /posts/2025/04/automatic-differentiation/
tags:
  - basic knowledge
  - Optimization Method
  - Machine Learning
---

最近最优化方法这门课上提到一些机器学习相关建模，包括**AD**,PCA,SVM,DNN等，几周前便想写一篇博客用于记录，但苦于没有时间。今天时间充足，尝试写一点。不求完美，后续有新的想法再补充。

自动微分是什么？
======
如我前言，**自动微分(Automatic Differentiation)**,简称AD。首先，请允许我做一些说明：**自动微分**的“微分”与微积分中的“微分”有差异！在微积分中，微分(Differentiation)，是函数在某一点处的线性近似增量，用导数与自变量微分的乘积表示，核心是“以线性逼近非线性”，忽略高阶无穷小。而在这里，自动微分中的“微分”一词，个人认为实际指“求导运算”，不是 dy,`differentiation as a process of computing derivatives`. 自动(Automatic),是相对手动(manial)而言，也就是手动计算。所以，自动微分就是使用计算机编写程序，“自动”求解函数在某一处的导数/梯度。自动微分可用于计算神经网络反向传播的梯度大小，是机器学习训练中不可获取的一步。后续行文中，“`微分`”的说法，实质上是指`计算导数`。


如何微分？
======
也就是如何求导？我相信在高中时，大家都会这件事了吧！记得当时用得最多的办法就是**记住**常用的基本初等函数求导公式。当然，你的数学老师可能进行了一些简单公式的推导。这里提供一个链接，提供了[基本初等函数求导公式](https://math.fandom.com/zh/wiki/%E5%9F%BA%E6%9C%AC%E5%88%9D%E7%AD%89%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0)，以及利用比值定义式取极限进行证明。这些公式里，有的很好记忆，例如幂函数，但大部分比较难记忆，例如，三角/反三角函数，双曲函数等...如果你不是经常使用，我相信大部分人会很快忘记的。但好在现在有了新工具帮助我们解决这个难题————**自动微分**。在TensorFlow、PyTorch都实现了自动微分，使用时直接调包即可。不过，作为计算机专业兼人工智能专业学生，学习其原理是十分有必要的。

为了更好的理解“自动微分”，我们需要先理解常见的求解微分的方式。（如前文所说，这里的“微分”是指“求导运算”）可分为一下四种：
- Manual Differentiation
- Numerical Differetiation
- Symbolic Differentiation
- Automatic Differentiayion
这里我就不做中文翻译了，感觉不管如何翻译，都会存在一定的理解偏差，“自动微分”就是个很好的例子（当然是指在翻译导致理解偏差这件事上）。

Manual Differentiation
------
`Manual Differentiation`,就是手算出求导结果，然后将公式编写成代码。例如
$$
f(x) = 3*x^5+4*x^2
$$
根据前述的基本初等函数求导公式，不难算出导函数为
$$
f'(x) = 15*x^4+8^x
$$
然后我们再把该公式用代码写出，
```cpp
f(x) = pow(3*x,5) + pow(4*x,2);
fd(x) = pow(15*x,4) + 8*x;
```
这样我们就能让计算机实现“自动微分”了，但这还不是我们今天讲的真正意义上的`自动微分`。因为存在至少有以下几个缺点：

- 代码复用性差。例如，如果我需要处理 $f(x) = 2*x^5+4*x^2$ 时，就需要重新运算，并修改代码。

- 复杂函数求导。这里有两层含义：第一是指函数本身复杂，例如对数、三角函数、指数函数的复合函数，很难保证自己的运算过程和结果是正确的；第二是指多变量情形，也就是对应的求梯度，当存在若干分量时，即使每一个分量求导很简单，但你需要重复很多次，这是多么让人懊恼的事！

对于第二个缺点的第一种情形，有人提出了`Numerical Differetiation`.

Numerical Differetiation
------
当我们面对实际计算问题时，往往是计算函数在某点的导数值，而不是导函数。因此，如果我们只想着先求出导函数，再代入对应的变量值，岂不是把路堵死了？根据导数的定义：
$$
f'(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
$$
,
只需要在 $x\_0$ 附近找到一个很小的 $\Delta x$ ，例如 `0.00000001`,
分别计算 $f(x + \Delta x)$ 和 $f(x)$ 的值，然后代入定义式，就能近似计算出 $x\_0$ 的导数值。可能你已经注意到，无论 $ f(x) $ 多么复杂，都可以依靠此方式获得一个不错的估计值（这里指很接近真实值）。但该方法也存在问题，计算中涉及多位小数，有`roundoff`和`truncation error`的风险。当计算要求的精度较高时，此方法便失效了；此外，如果换成另外一个点的导数值，又需重新计算。

Symbolic Differentiation
------
我所理解的`Symbolic Differentiation`，实际上就是微积分中的一些微积分技巧（trick），例如链式求导法则：
$$
\frac{d}{dx}(f(x)+g(x)) = \frac{d}{dx}f(x)+\frac{d}{dx}g(x) \\
\frac{d}{dx}(f(x)g(x)) = (\frac{d}{dx}f(x))g(x)+f(x)(\frac{d}{dx}g(x)) 
$$
通过符号(symbol)求出导函数的“闭式”(closed form)解析形式。但并不是所有的导函数都能求出闭式。


自动微分(Automatic Differrentiation)
------
